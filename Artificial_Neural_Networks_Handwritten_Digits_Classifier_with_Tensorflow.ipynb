{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Artificial Neural Networks - Handwritten Digits Classifier with Tensorflow",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4H_cenmpKZ_",
        "colab_type": "text"
      },
      "source": [
        "## Artificial Neural Networks for Handwritten Digits Classification\n",
        "\n",
        "In this module we will develop another multilayer perceptrion, but now we will dive up in the tensorflow library. We will keep following the same project structure:\n",
        "\n",
        "1. Loading Data\n",
        "2. Exploring Data\n",
        "3. Data preprocessing: Rescaling (with one hot encoding) and Validation dataset\n",
        "4. Set up the model\n",
        "5. Deploying the model\n",
        "6. Testing our model\n",
        "\n",
        "**NOTE:** Check the Artificial Neural Networks and the Image Classifier example before reading this notebook. There are several extremely important foundation concepts for you. **In this notebook we will focus on Tensorflow and not in the Neural Networks basic concepts.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-YmYVSConss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "from numpy.random import seed\n",
        "seed(888)\n",
        "tf.random.set_seed(404)\n",
        "\n",
        "from time import strftime\n",
        "from PIL import Image \n",
        "\n",
        "# Constants\n",
        "\n",
        "X_TRAIN_PATH = '/content/drive/My Drive/digit_xtrain.csv'\n",
        "X_TEST_PATH = '/content/drive/My Drive/digit_xtest.csv'\n",
        "Y_TRAIN_PATH = '/content/drive/My Drive/digit_ytrain.csv'\n",
        "Y_TEST_PATH = '/content/drive/My Drive/digit_ytest.csv'\n",
        "LOGGING_PATH = '/content/drive/My Drive/Tensorboard_mnist_digit_logs'\n",
        "\n",
        "\n",
        "IMAGE_WIDTH = 28\n",
        "IMAGE_HEIGHT = 28\n",
        "COLOUR_CHANNELS = 1\n",
        "TOTAL_INPUTS = IMAGE_HEIGHT * IMAGE_WIDTH * COLOUR_CHANNELS"
      ],
      "execution_count": 392,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh7aKRazt9nC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 1. Loading the data\n",
        "y_train_all = np.loadtxt(Y_TRAIN_PATH, dtype=int, delimiter=',')\n",
        "y_test_all = np.loadtxt(Y_TEST_PATH, dtype=int, delimiter=',')\n",
        "\n",
        "x_train_all = np.loadtxt(X_TRAIN_PATH, dtype=int, delimiter=',')\n",
        "x_test_all = np.loadtxt(X_TEST_PATH, dtype=int, delimiter=',')"
      ],
      "execution_count": 280,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sj5SNJnIuMun",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "89132848-3d62-4fc2-8805-e9d723a3e945"
      },
      "source": [
        "# 2. Data Exploration \n",
        "# We are working with images from handwritten digits in grayscale. Measurements: (28 x 28 x 1 - single color channel) = 784\n",
        "\n",
        "print('y train shape:', y_train_all.shape) # 60.000 labels to go along with our x train featues\n",
        "print('y test shape:', y_test_all.shape) # 10.000 labels for testing our results\n",
        "\n",
        "print('x train shape:', x_train_all.shape) # 60.000 images examples and each example 784 values associated (size of the image)\n",
        "print('x test shape:', x_test_all.shape) # 10.000 images for testing and each one has 784 values associated (size of the image)"
      ],
      "execution_count": 281,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y train shape: (60000,)\n",
            "y test shape: (10000,)\n",
            "x train shape: (60000, 784)\n",
            "x test shape: (10000, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXF-Y7rRycpJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61f8c2a8-0283-4445-a18b-9f34c4091973"
      },
      "source": [
        "# Dive in the array with the first example:\n",
        "x_train_all[0]"
      ],
      "execution_count": 282,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   3,  18,  18,  18,\n",
              "       126, 136, 175,  26, 166, 255, 247, 127,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170, 253,\n",
              "       253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253,\n",
              "       253, 253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 219, 253,\n",
              "       253, 253, 253, 253, 198, 182, 247, 241,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "        80, 156, 107, 253, 253, 205,  11,   0,  43, 154,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,  14,   1, 154, 253,  90,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0, 139, 253, 190,   2,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190, 253,  70,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "       241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,  81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,  45, 186, 253, 253, 150,  27,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,  16,  93, 252, 253, 187,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 249,\n",
              "       253, 249,  64,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  46, 130,\n",
              "       183, 253, 253, 207,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39, 148,\n",
              "       229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114,\n",
              "       221, 253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  23,  66,\n",
              "       213, 253, 253, 253, 253, 198,  81,   2,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  18, 171,\n",
              "       219, 253, 253, 253, 253, 195,  80,   9,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  55, 172,\n",
              "       226, 253, 253, 253, 253, 244, 133,  11,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "       136, 253, 253, 253, 212, 135, 132,  16,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         0,   0,   0,   0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 282
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEkfCGtJyxin",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "81e8fb88-29bf-4f30-beeb-200d3bcbe2d0"
      },
      "source": [
        "# Checking our y_train_all labels\n",
        "print('First y_train label is:', y_train_all[0]) # The answer is '5' for the first example\n",
        "print('Five first y_train labels are:', y_train_all[:5]) \n",
        "\n",
        "# Note that our array is already flattened, because we have the 784 shape. If there was not flattened the size would be 10000, 28, 28, 1.\n",
        "# Good: It is easy to work\n",
        "# Bad: We loose a little bit information positioning\n"
      ],
      "execution_count": 283,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First y_train label is: 5\n",
            "Five first y_train labels are: [5 0 4 1 9]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31C7ZXrmzX78",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 3. Data Preprocessing\n",
        "\n",
        "# Rescaling our data: >> we will transform our dataset to 0 to 1 scale instead 0 - 255 to make our future calculations easier.\n",
        "x_train_all, x_test_all = x_train_all / 255.0 , x_test_all / 255.0"
      ],
      "execution_count": 284,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoHuBEFv2N6d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "96818c59-ed10-43d9-dcda-f952431f75c3"
      },
      "source": [
        "# One-Hot Encoding >> Transforming our data to 0 - 1 scale, in which each column is a feature and in the answer will be assgined with 0 or 1. Good for categorical data\n",
        "# We will transform our sparse matrix (5, 0, 4, 1, 9...) into a full matrix (each column will be the answer containing 0 or 1)\n",
        "\n",
        "# Example:\n",
        "values = y_train_all[:5]\n",
        "\n",
        "# Build a 2-D array with 1's and zeroes given some pattern. By defaul the diagonal is 1.\n",
        "np.eye(10)[values] \n",
        "\n",
        "# np.eye(N ==> amount of rows, M ==> amount of columns)[index pattern] \n",
        "# Similarly if we use np.eye(10)[2] we will have a 10 x 10 matrix with '1' in the index 2 for each row. Other ones will be zero "
      ],
      "execution_count": 285,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 285
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjsRTxfd3Gv_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8b4291b4-6796-4801-8fd8-5509468470fc"
      },
      "source": [
        "# Applying to our dataset\n",
        "\n",
        "NR_CLASSES = 10 # 0 - 9 digit = 10 classes in our dataset.\n",
        "\n",
        "y_train_all = np.eye(NR_CLASSES)[y_train_all]\n",
        "\n",
        "print('y train reshaped:', y_train_all.shape) # 60.000 examples (features) + 10 columns one hot encoded"
      ],
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y train reshaped: (60000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIVGxC5h5UFW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a7cdf217-68ea-4d51-aa33-1878cf0425d2"
      },
      "source": [
        "y_test_all = np.eye(NR_CLASSES)[y_test_all]\n",
        "\n",
        "print('y test reshaped:', y_test_all.shape) # 10.000 examples (features) + 10 columns one hot encoded"
      ],
      "execution_count": 287,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "y test reshaped: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7TK6Q0l5aBT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "8cd26115-1a3b-466d-f66a-9ec4073887c1"
      },
      "source": [
        "# Creating the validation dataset\n",
        "\n",
        "VALIDATION_SIZE = 10000 # Same size of our test set\n",
        "\n",
        "x_validation = x_train_all[:VALIDATION_SIZE]\n",
        "y_validation = y_train_all[:VALIDATION_SIZE]\n",
        "\n",
        "x_train_xs = x_train_all[VALIDATION_SIZE:]\n",
        "y_train_xs = y_train_all[VALIDATION_SIZE:]\n",
        "\n",
        "print('x train new shape:', x_train_xs.shape)\n",
        "print('y train new shape:', y_train_xs.shape)\n",
        "\n",
        "print('x validation shape:', x_validation.shape)\n",
        "print('y validation shape:', y_validation.shape)"
      ],
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x train new shape: (50000, 784)\n",
            "y train new shape: (50000, 10)\n",
            "x validation shape: (10000, 784)\n",
            "y validation shape: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpVfDzjZ8PwO",
        "colab_type": "text"
      },
      "source": [
        "### Tensorflow Guideline\n",
        "\n",
        "What is a Tensor? Tensor is essentialy a way to store data in a specific structure. **Basically is a container with n-dimensions called 'ranks'**.\n",
        "\n",
        "They follow some mathematical rules, so you can add, multiply, and make several linear algebra operations like we did with numpy. This is handy when working with Neural Networks given the calculations that we will need to use in the input and hidden layers and so on.\n",
        "\n",
        "Differently from Scikit-Learn models and Keras frameworks, **Tensorflow allows us to develop by ourselves all steps during the construction of neural network. We can deal with the inner calculations that are abstracted with the Keras/Scikit-Learn frameworks**. With tensorflow we can define how our weights and biases will work, how the layers will behave, and **how is the FLOW between layers (that's why is called tensorflow! the flow between tensors!)** etc.\n",
        "\n",
        "Let's use the architecture sketch below in order to build our multilayer perceptron. We will have inputs from some layer that must be multiplied to outputs for another layer (imagine input hidden or output layers for an ordinary neural network).\n",
        "\n",
        "(IMAGE HERE)\n",
        "\n",
        "We will define how our weights and biases will be calculated, we will need to set a matrix multiplication between the output and input. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0INeuOdGjLgC",
        "colab_type": "text"
      },
      "source": [
        "### Setting up our Tensorflow Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cD18pine7D6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Set up Tensorflow Graph\n",
        "\n",
        "# Disabling eager execution for compatibility with TF 2.0  >> Change the code for TF 2.0\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "# Creating our placeholder - tensor\n",
        "X = tf.compat.v1.placeholder(tf.float32, shape=[None, TOTAL_INPUTS]) # Dimensions: None >> Examples/Samples to be used in the tensor we will define later because they will be divided in the batches, 784 features\n",
        "y = tf.compat.v1.placeholder(tf.float32, shape=[None, NR_CLASSES]) # Dimensions: None , 10 = number of categories that we wanna predict"
      ],
      "execution_count": 459,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NlZELV6h18H",
        "colab_type": "text"
      },
      "source": [
        "### Setting up our Neural Network Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WeAghPBgAEzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Artificial Neural Network Architecture\n",
        "\n",
        "# 1. Hyperparameters\n",
        "nr_epocs = 50\n",
        "learning_rate = 0.001 # or 1e-3 / 1e-4 \n",
        "\n",
        "# 2. Layers will be set in this neural network\n",
        "n_hidden1 = 512 # amount of neurons\n",
        "n_hidden2 = 64 # amount of neurons\n",
        "# Output layer will have 10 neurons"
      ],
      "execution_count": 460,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sY7vGxTiLXPU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def setup_layer(input, weight_dim, bias_dim, name):\n",
        "\n",
        "  with tf.name_scope(name):\n",
        "\n",
        "    # A. Weights setup\n",
        "    initial_w = tf.compat.v1.truncated_normal(shape=weight_dim, stddev= 0.1, seed= 42) # shape = [n_hidden2, NR_CLASSES]\n",
        "    w = tf.compat.v1.Variable(initial_value=initial_w, name='W')\n",
        "\n",
        "    # B. Biases setup\n",
        "    initial_b = tf.compat.v1.constant(value=0.0, shape=bias_dim)\n",
        "    b = tf.compat.v1.Variable(initial_value=initial_b, name='B')\n",
        "\n",
        "    # C. How Layer will work\n",
        "    # Layer - Inputs and Calculations\n",
        "    layer_in = tf.linalg.matmul(input, w) + b # Input = results from the previous layer. \n",
        "\n",
        "    # Layer - Output - Activation Function\n",
        "    if name == 'output':\n",
        "\n",
        "      layer_out = tf.nn.softmax(layer_in)\n",
        "    \n",
        "    else: \n",
        "\n",
        "      layer_out = tf.nn.relu(layer_in)\n",
        "    \n",
        "    # Tensorboard\n",
        "    tf.summary.histogram('weights', w) # Summary for weights\n",
        "    tf.summary.histogram('biases', b) # Summary for weights\n",
        "    \n",
        "    return layer_out\n",
        "    "
      ],
      "execution_count": 461,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoUc4uHvNr46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Architecture \n",
        "\n",
        "layer_1 = setup_layer(X, \n",
        "                      weight_dim=[TOTAL_INPUTS, n_hidden1], \n",
        "                      bias_dim= [n_hidden1], \n",
        "                      name='layer_1')\n",
        "\n",
        "layer_2 = setup_layer(layer_1, \n",
        "                      weight_dim=[n_hidden1, n_hidden2], \n",
        "                      bias_dim= [n_hidden2], \n",
        "                      name='layer_2')\n",
        "\n",
        "output = setup_layer(layer_2, \n",
        "                     weight_dim=[n_hidden2, NR_CLASSES], \n",
        "                     bias_dim= [NR_CLASSES], \n",
        "                     name='output')\n",
        "\n",
        "model_name = f'{n_hidden1}-{n_hidden2}-Learning Rate:{learning_rate} Epochs:{nr_epocs}'\n"
      ],
      "execution_count": 462,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0EWfeRX8qSW9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "6148f8d8-fd36-459c-d7b5-b4824521f19b"
      },
      "source": [
        "# Model Architecture with Dropouts\n",
        "\"\"\"\n",
        "layer_1 = setup_layer(X, \n",
        "                      weight_dim=[TOTAL_INPUTS, n_hidden1], \n",
        "                      bias_dim= [n_hidden1], \n",
        "                      name='layer_1')\n",
        "\n",
        "# Tensorflow 1.x >> layer_dropout = tf.compat.v1.nn.dropout(layer_1, keep_prob=0.8, name='dropout_layer')\n",
        "# Tensorflow 2.0\n",
        "layer_dropout= tf.nn.dropout(layer_1, 0.8) # Parameters: Previous Layer + % of dropout \n",
        "\n",
        "layer_2 = setup_layer(layer_dropout, \n",
        "                      weight_dim=[n_hidden1, n_hidden2], \n",
        "                      bias_dim= [n_hidden2], \n",
        "                      name='layer_2')\n",
        "\n",
        "output = setup_layer(layer_2, \n",
        "                     weight_dim=[n_hidden2, NR_CLASSES], \n",
        "                     bias_dim= [NR_CLASSES], \n",
        "                     name='output')\n",
        "\n",
        "model_name = f'{n_hidden1}-{n_hidden2}-Dropout-Learning Rate:{learning_rate} Epochs:{nr_epocs}'\n",
        "\"\"\"\n",
        "\n"
      ],
      "execution_count": 463,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nlayer_1 = setup_layer(X, \\n                      weight_dim=[TOTAL_INPUTS, n_hidden1], \\n                      bias_dim= [n_hidden1], \\n                      name='layer_1')\\n\\n# Tensorflow 1.x >> layer_dropout = tf.compat.v1.nn.dropout(layer_1, keep_prob=0.8, name='dropout_layer')\\n# Tensorflow 2.0\\nlayer_dropout= tf.nn.dropout(layer_1, 0.8) # Parameters: Previous Layer + % of dropout \\n\\nlayer_2 = setup_layer(layer_dropout, \\n                      weight_dim=[n_hidden1, n_hidden2], \\n                      bias_dim= [n_hidden2], \\n                      name='layer_2')\\n\\noutput = setup_layer(layer_2, \\n                     weight_dim=[n_hidden2, NR_CLASSES], \\n                     bias_dim= [NR_CLASSES], \\n                     name='output')\\n\\nmodel_name = f'{n_hidden1}-{n_hidden2}-Dropout-Learning Rate:{learning_rate} Epochs:{nr_epocs}'\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 463
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADEagCLAO7II",
        "colab_type": "text"
      },
      "source": [
        "## Step by step intuition\n",
        "\n",
        "The function setup_layer summarizes the codepads below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyD6YJjHKy9Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "2c9a905c-cf8a-4f99-86d8-6c169dc96b76"
      },
      "source": [
        "                                  ############## Layer 1 Architecture ##############\n",
        "\"\"\"\n",
        "\n",
        "with tf.name_scope('hidden_1'):\n",
        "######### Weights and Biases initial guess (random values). With tensorflow we can pick them within a distribuition ##############\n",
        "\n",
        "  # A. Initial weights in the first hidden layer >> Remember they are responsible for flatting or sheeping our activation function\n",
        "  # This function will follow the normal distribuition truncating outliers from tails. \n",
        "  initial_w1 = tf.compat.v1.truncated_normal(shape=[TOTAL_INPUTS, n_hidden1], # Shape = inputs and neurons in the layer\n",
        "                                            stddev= 0.1, # How to pick our guesses? Deviation by 0.1\n",
        "                                            seed= 42) \n",
        "\n",
        "  # Important: We can't display our initial weights yet. Tensorflow works by stages: 1st stage set up ; 2nd how to perform calculations >> After that we can see our objects\n",
        "\n",
        "  # Creating a tensorflow variable, because our weights must be updated later\n",
        "  w1 = tf.compat.v1.Variable(initial_value=initial_w1)\n",
        "\n",
        "\n",
        "\n",
        "  # B. Initializing biases >> Remember they will be responsible for shifiting our activation function.\n",
        "  initial_b1 = tf.compat.v1.constant(value=0.0, # initial value will be zero, in other words no biases at the beginning\n",
        "                                    shape=[n_hidden1]) # shape of our biases\n",
        "\n",
        "  # Creating the same tensorflow variable\n",
        "  b1 = tf.compat.v1.Variable(initial_value=initial_b1)\n",
        "\n",
        "\n",
        "  # C. Determining how the weights and biases will work in our neural network and defining the flow control between inputs/outputs\n",
        "\n",
        "  # Layer 1 - Inputs and Calculations\n",
        "  layer1_in = tf.linalg.matmul(X, w1) + b1 # INPUT LAYER + HIDDEN LAYER = Multiply our X inputs with the weights + bias in the first layer \n",
        "\n",
        "  # Layer 1 - Output - Activation Function\n",
        "  layer1_out = tf.nn.relu(layer1_in) # HIDDEN LAYER (512 neurons hidden layer)\n",
        "\"\"\""
      ],
      "execution_count": 464,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n\\nwith tf.name_scope('hidden_1'):\\n######### Weights and Biases initial guess (random values). With tensorflow we can pick them within a distribuition ##############\\n\\n  # A. Initial weights in the first hidden layer >> Remember they are responsible for flatting or sheeping our activation function\\n  # This function will follow the normal distribuition truncating outliers from tails. \\n  initial_w1 = tf.compat.v1.truncated_normal(shape=[TOTAL_INPUTS, n_hidden1], # Shape = inputs and neurons in the layer\\n          stddev= 0.1, # How to pick our guesses? Deviation by 0.1\\n          seed= 42) \\n\\n  # Important: We can't display our initial weights yet. Tensorflow works by stages: 1st stage set up ; 2nd how to perform calculations >> After that we can see our objects\\n\\n  # Creating a tensorflow variable, because our weights must be updated later\\n  w1 = tf.compat.v1.Variable(initial_value=initial_w1)\\n\\n\\n\\n  # B. Initializing biases >> Remember they will be responsible for shifiting our activation function.\\n  initial_b1 = tf.compat.v1.constant(value=0.0, # initial value will be zero, in other words no biases at the beginning\\n  shape=[n_hidden1]) # shape of our biases\\n\\n  # Creating the same tensorflow variable\\n  b1 = tf.compat.v1.Variable(initial_value=initial_b1)\\n\\n\\n  # C. Determining how the weights and biases will work in our neural network and defining the flow control between inputs/outputs\\n\\n  # Layer 1 - Inputs and Calculations\\n  layer1_in = tf.linalg.matmul(X, w1) + b1 # INPUT LAYER + HIDDEN LAYER = Multiply our X inputs with the weights + bias in the first layer \\n\\n  # Layer 1 - Output - Activation Function\\n  layer1_out = tf.nn.relu(layer1_in) # HIDDEN LAYER (512 neurons hidden layer)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 464
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us0kyHazEMnH",
        "colab_type": "text"
      },
      "source": [
        "**Tensorflow Variables**: Imagine a variable like something that preserves the state of the graph. They can be updated and changed! for this reason w1 and b1 are variables. We need to initialize all variables, that's why we will assign 'global_variables_initializer()' later on.\n",
        "\n",
        "**Constants:** Values that don't change over time"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0aGnrriTHnR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "63b4f0db-79ba-4ab8-ae3c-6bb04728f76e"
      },
      "source": [
        "                                  ############## Layer 2 Architecture ##############\n",
        "\"\"\"\n",
        "with tf.name_scope('hidden_layer_2'):\n",
        "  # A. Weights setup\n",
        "  initial_w2 = tf.compat.v1.truncated_normal(shape=[n_hidden1, n_hidden2], # Shape = nr of neurons from the previous layer and neurons in the current layer\n",
        "                                            stddev= 0.1, # How to pick our guesses? Deviation by 0.1\n",
        "                                            seed= 42) \n",
        "\n",
        "  w2 = tf.compat.v1.Variable(initial_value=initial_w2)\n",
        "\n",
        "  # B. Biases setup\n",
        "  initial_b2 = tf.compat.v1.constant(value=0.0, # initial value will be zero, in other words no biases at the beginning\n",
        "                                    shape=[n_hidden2]) # shape of our biases\n",
        "\n",
        "  b2 = tf.compat.v1.Variable(initial_value=initial_b2)\n",
        "\n",
        "\n",
        "  # C. How Layer 2 will work\n",
        "  # Layer 2 - Inputs and Calculations\n",
        "  layer2_in = tf.linalg.matmul(layer1_out, w2) + b2 # 1st HIDDEN LAYER OUTPUT (512 neurons) + 2nd HIDDEN LAYER (64 neurons) = Multiply our X inputs with the weights + bias in the previous layer \n",
        "\n",
        "  # Layer 2 - Output - Activation Function\n",
        "  layer2_out = tf.nn.relu(layer2_in) # 2nd HIDDEN LAYER (64 neurons hidden layer)\n",
        "\"\"\""
      ],
      "execution_count": 465,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"\\nwith tf.name_scope('hidden_layer_2'):\\n  # A. Weights setup\\n  initial_w2 = tf.compat.v1.truncated_normal(shape=[n_hidden1, n_hidden2], # Shape = nr of neurons from the previous layer and neurons in the current layer\\n          stddev= 0.1, # How to pick our guesses? Deviation by 0.1\\n          seed= 42) \\n\\n  w2 = tf.compat.v1.Variable(initial_value=initial_w2)\\n\\n  # B. Biases setup\\n  initial_b2 = tf.compat.v1.constant(value=0.0, # initial value will be zero, in other words no biases at the beginning\\n  shape=[n_hidden2]) # shape of our biases\\n\\n  b2 = tf.compat.v1.Variable(initial_value=initial_b2)\\n\\n\\n  # C. How Layer 2 will work\\n  # Layer 2 - Inputs and Calculations\\n  layer2_in = tf.linalg.matmul(layer1_out, w2) + b2 # 1st HIDDEN LAYER OUTPUT (512 neurons) + 2nd HIDDEN LAYER (64 neurons) = Multiply our X inputs with the weights + bias in the previous layer \\n\\n  # Layer 2 - Output - Activation Function\\n  layer2_out = tf.nn.relu(layer2_in) # 2nd HIDDEN LAYER (64 neurons hidden layer)\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 465
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Oleo2W5ZasE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171
        },
        "outputId": "72f452e4-c1d7-4383-d819-ef70394d3369"
      },
      "source": [
        "                                  ############## Output Layer Architecture ##############\n",
        "\"\"\"\n",
        "# Name Scope: Helps us to organize our code. All the weights, biases, calculations are in the same context from 'Output Layer'\n",
        "# Our Tensorflow Graph will summurize all these calculations into just one scope. \n",
        "with tf.name_scope('output_layer'):\n",
        "\n",
        "  # A. Weights setup\n",
        "  initial_w3 = tf.compat.v1.truncated_normal(shape=[n_hidden2, NR_CLASSES], # Shape = nr of neurons from the previous layer , neurons in the current layer\n",
        "                                            stddev= 0.1, # How to pick our guesses? Deviation by 0.1\n",
        "                                            seed= 42) \n",
        "\n",
        "  w3 = tf.compat.v1.Variable(initial_value=initial_w3)\n",
        "\n",
        "  # B. Biases setup\n",
        "  initial_b3 = tf.compat.v1.constant(value=0.0, # initial value will be zero, in other words no biases at the beginning\n",
        "                                    shape=[NR_CLASSES]) # shape of our biases given the current output\n",
        "\n",
        "  b3 = tf.compat.v1.Variable(initial_value=initial_b3)\n",
        "\n",
        "\n",
        "  # C. How Layer 2 will work\n",
        "  # Layer 2 - Inputs and Calculations\n",
        "  layer3_in = tf.linalg.matmul(layer2_out, w3) + b3 # 1st HIDDEN LAYER OUTPUT (512 neurons) + 2nd HIDDEN LAYER (64 neurons) = Multiply our X inputs with the weights + bias in the previous layer \n",
        "\n",
        "  # Layer 2 - Output - Activation Function\n",
        "  output = tf.nn.softmax(layer3_in) # OUTPUT LAYER (10 neurons) / Softmax = Probabilistic answer\n",
        "\"\"\""
      ],
      "execution_count": 466,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic": {
              "type": "string"
            },
            "text/plain": [
              "\"\\n# Name Scope: Helps us to organize our code. All the weights, biases, calculations are in the same context from 'Output Layer'\\n# Our Tensorflow Graph will summurize all these calculations into just one scope. \\nwith tf.name_scope('output_layer'):\\n\\n  # A. Weights setup\\n  initial_w3 = tf.compat.v1.truncated_normal(shape=[n_hidden2, NR_CLASSES], # Shape = nr of neurons from the previous layer , neurons in the current layer\\n          stddev= 0.1, # How to pick our guesses? Deviation by 0.1\\n          seed= 42) \\n\\n  w3 = tf.compat.v1.Variable(initial_value=initial_w3)\\n\\n  # B. Biases setup\\n  initial_b3 = tf.compat.v1.constant(value=0.0, # initial value will be zero, in other words no biases at the beginning\\n  shape=[NR_CLASSES]) # shape of our biases given the current output\\n\\n  b3 = tf.compat.v1.Variable(initial_value=initial_b3)\\n\\n\\n  # C. How Layer 2 will work\\n  # Layer 2 - Inputs and Calculations\\n  layer3_in = tf.linalg.matmul(layer2_out, w3) + b3 # 1st HIDDEN LAYER OUTPUT (512 neurons) + 2nd HIDDEN LAYER (64 neurons) = Multiply our X inputs with the weights + bias in the previous layer \\n\\n  # Layer 2 - Output - Activation Function\\n  output = tf.nn.softmax(layer3_in) # OUTPUT LAYER (10 neurons) / Softmax = Probabilistic answer\\n\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 466
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YckbrCYKLYCW",
        "colab_type": "text"
      },
      "source": [
        "### Note that our code before is written a little bit repeatedly, why not define a function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eq3Fitsg1S8S",
        "colab_type": "text"
      },
      "source": [
        "### Tensorboard Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxTq0mQt1XFG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3e7d5dfe-3941-4cb3-9cb2-fa93472383fd"
      },
      "source": [
        "## Tensorboard Setup\n",
        "\n",
        "folder_name = f'Model {model_name} at {strftime(\"%H:%M\")}' # For each model creates a specific folder\n",
        "directory = os.path.join(LOGGING_PATH, folder_name)\n",
        "\n",
        "try:\n",
        "\n",
        "  os.makedirs(directory)\n",
        "\n",
        "except OSError as e:\n",
        "\n",
        "  print(e)\n",
        "\n",
        "else:\n",
        "\n",
        "  print(\"Directory created successfully\")\n",
        "\n",
        "# Tensorboard needs to create summaries from our metrics. For this reason we will need to bound our metrics with the tensorboard"
      ],
      "execution_count": 467,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Directory created successfully\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQfhTmPgaRrO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                                  ############## Loss, Optimizer and Metrics ##############\n",
        "\n",
        "# A. Loss\n",
        "# Cross Entropy Loss >> Given the classification task is the best function to calculate our loss.\n",
        "# logits = Outputs from our Output Layer \n",
        "# labels = Acctual Labels (y values)\n",
        "\n",
        "# Tensorflow 2.0 >> loss = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output)\n",
        "\n",
        "with tf.name_scope('loss_calc'):\n",
        "  # We will calculate our model with Batches, for this reason we need to take the avg\n",
        "  loss = tf.reduce_mean(tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(labels=y, logits=output))        \n",
        "\n",
        "\n",
        "# B. Optimizer\n",
        "with tf.name_scope('optimizer'):\n",
        "  # Set up our optimizer with our learning rate (by default is 1)\n",
        "  optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate= learning_rate) \n",
        "\n",
        "  # Update the values from the previous layers (Backpropagation) by minimizing our loss (Note: If you use uphill approach with SGD you'll need to adapt here)\n",
        "  train_step = optimizer.minimize(loss)\n",
        "\n",
        "\n",
        "# C. Metrics - Accuracy\n",
        "with tf.name_scope('accuracy_calc'):\n",
        "  correct_prediction = tf.compat.v1.equal(tf.argmax(output, axis=1), tf.argmax(y, axis=1)) # We need to adapt given that the one hot encoded our outputs!\n",
        "  accuraccy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) # get the mean given that we are working with batches!"
      ],
      "execution_count": 468,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3VEtASEX4FeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.name_scope('performance'):\n",
        "  # Adding 'accuracy' metric to tensorboard\n",
        "  tf.compat.v1.summary.scalar('accuraccy', accuraccy)\n",
        "  tf.compat.v1.summary.scalar('loss', loss)"
      ],
      "execution_count": 469,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e5BkbByV5G3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checking images with Tensorboard\n",
        "\n",
        "with tf.name_scope('show_image'):\n",
        "\n",
        "  # 1º we need to reshape for a 4-D tensor\n",
        "  x_image = tf.reshape(X,[-1, 28, 28, 1]) # 28 x 28 x 1 + 1 new dimension (4-D) dummy\n",
        "  tf.compat.v1.summary.image('image_put', x_image, max_outputs=4)"
      ],
      "execution_count": 470,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ecEVxivBhyFE",
        "colab_type": "text"
      },
      "source": [
        "### Training our model (Sessions, Batching and Training)\n",
        "\n",
        "Tensorflow works with Session objects, that are responsible for encapsulate the operations and calculations. \n",
        "\n",
        "In the Session that we start all variables, and manage all the placeholders and flow from our code/graph. Placeholders must be fed with data during the session, for this reason we placed the dictionary in our batch function!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3RDfe3I2dgOn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Sessions, Batching and Training\n",
        "\n",
        "# A. Sessions - Defining our session and initializing all variables previously set up\n",
        "sess = tf.compat.v1.Session()\n",
        "\n",
        "# Initialize global variables\n",
        "init = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "# Tensorboard step\n",
        "merged_summary = tf.compat.v1.summary.merge_all() # Merge all summaries like a global collector\n",
        "train_writer = tf.compat.v1.summary.FileWriter(directory + '/train') # File Writer: Manager to write down data and set the local to save our training summary files\n",
        "train_writer.add_graph(sess.graph) # All calculations from Tensorflow are graphs. so we need to redirect from Tensorboard our session Graph\n",
        "\n",
        "# Validation writer! We don't need to append the graph because they use the same session\n",
        "validation_writer = tf.compat.v1.summary.FileWriter(directory + '/validation')\n",
        "\n",
        "\n",
        "\n",
        "# Binding our variables to the session \n",
        "sess.run(init)\n",
        "\n",
        "# After running we can check our previous variables! w1, b1, etc."
      ],
      "execution_count": 471,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GStRM9wjdSz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checking our starting weights\n",
        "#w1.eval(sess)"
      ],
      "execution_count": 472,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSJDEmNPjft-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checking our starting biases\n",
        "#b1.eval(sess)"
      ],
      "execution_count": 473,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygKsB7IajrgU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batching the Data >> Split up our training dataset into small batches for better performance!\n",
        "\n",
        "size_of_batch = 1000\n",
        "\n",
        "num_examples = y_train_xs.shape[0]\n",
        "nr_iterations = int(num_examples / size_of_batch) # 50 \n",
        "\n",
        "index_in_epoch = 0 # Keep tracking that the 1st epoch will start with the index 0 - 999. 2nd batch 1000 - 1999 and so onsiz"
      ],
      "execution_count": 474,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Um5zJ1H2kwU6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function to transfer the current batch to the next batch\n",
        "def next_batch(batch_size, data, labels):\n",
        "\n",
        "  global num_examples # global collects the variable that is outside this context\n",
        "  global index_in_epoch\n",
        "\n",
        "  # Batch Start Controller\n",
        "  start = index_in_epoch\n",
        "  \n",
        "  # Updates our index in epoch with the size batch size\n",
        "  index_in_epoch += batch_size\n",
        "\n",
        "  if index_in_epoch > num_examples:\n",
        "\n",
        "    #start next epoch\n",
        "    start = 0\n",
        "    index_in_epoch = batch_size\n",
        "\n",
        "  # Batch End Controller\n",
        "  end = index_in_epoch\n",
        "\n",
        "  return data[start:end], labels[start:end] # Features and Labels\n"
      ],
      "execution_count": 475,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDwHL71Tm8WQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "83c93af5-b0db-4612-ce8f-0f7c5dbe1f47"
      },
      "source": [
        "# Training our model (Loop)\n",
        "\n",
        "for epoch in range(nr_epocs):\n",
        "\n",
        "  for i in range(nr_iterations):\n",
        "\n",
        "    batch_x, batch_y = next_batch(batch_size= size_of_batch,\n",
        "                                  data= x_train_xs,\n",
        "                                  labels= y_train_xs)\n",
        "    \n",
        "    # Feed dictionary >> Used to feed our session with data\n",
        "    feed_dictionary = {X: batch_x, y: batch_y} # X and y where defined in the tensorflow graph set up! Be minded that our placeholder and our data must have the same shape\n",
        "\n",
        "    # Execute our train step (Optimizer = Reducing Loss) given our data dictionary\n",
        "    sess.run(train_step, feed_dict= feed_dictionary)\n",
        "\n",
        "    # Accuracy: Fetches the calculated accuracy (like a callback)\n",
        "    #batch_accuracy = sess.run(fetches=[accuraccy], feed_dict= feed_dictionary) # add our merged_summary to create a summary each epoch\n",
        "\n",
        "  \n",
        "  # Tensorboard \n",
        "  s, batch_accuracy = sess.run(fetches=[merged_summary, accuraccy], feed_dict= feed_dictionary) # >>> Tensorboard\n",
        "  train_writer.add_summary(s, epoch) # Each epoch saves the summary 's'\n",
        "\n",
        "  print(f'Epoch {epoch} \\t| Training accuracy = {batch_accuracy}')\n",
        "\n",
        "  # Tensorboard - Validation \n",
        "  summary = sess.run(fetches=merged_summary, feed_dict={X:x_validation, y:y_validation})\n",
        "  validation_writer.add_summary(summary, epoch)\n",
        "\n",
        "print('Training finished')"
      ],
      "execution_count": 476,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0 \t| Training accuracy = 0.859000027179718\n",
            "Epoch 1 \t| Training accuracy = 0.8669999837875366\n",
            "Epoch 2 \t| Training accuracy = 0.8709999918937683\n",
            "Epoch 3 \t| Training accuracy = 0.875\n",
            "Epoch 4 \t| Training accuracy = 0.8759999871253967\n",
            "Epoch 5 \t| Training accuracy = 0.9620000123977661\n",
            "Epoch 6 \t| Training accuracy = 0.9829999804496765\n",
            "Epoch 7 \t| Training accuracy = 0.9860000014305115\n",
            "Epoch 8 \t| Training accuracy = 0.984000027179718\n",
            "Epoch 9 \t| Training accuracy = 0.9869999885559082\n",
            "Epoch 10 \t| Training accuracy = 0.9869999885559082\n",
            "Epoch 11 \t| Training accuracy = 0.9879999756813049\n",
            "Epoch 12 \t| Training accuracy = 0.9890000224113464\n",
            "Epoch 13 \t| Training accuracy = 0.9900000095367432\n",
            "Epoch 14 \t| Training accuracy = 0.9890000224113464\n",
            "Epoch 15 \t| Training accuracy = 0.9900000095367432\n",
            "Epoch 16 \t| Training accuracy = 0.9909999966621399\n",
            "Epoch 17 \t| Training accuracy = 0.9909999966621399\n",
            "Epoch 18 \t| Training accuracy = 0.9909999966621399\n",
            "Epoch 19 \t| Training accuracy = 0.9900000095367432\n",
            "Epoch 20 \t| Training accuracy = 0.9909999966621399\n",
            "Epoch 21 \t| Training accuracy = 0.9919999837875366\n",
            "Epoch 22 \t| Training accuracy = 0.9919999837875366\n",
            "Epoch 23 \t| Training accuracy = 0.9919999837875366\n",
            "Epoch 24 \t| Training accuracy = 0.9929999709129333\n",
            "Epoch 25 \t| Training accuracy = 0.9929999709129333\n",
            "Epoch 26 \t| Training accuracy = 0.9929999709129333\n",
            "Epoch 27 \t| Training accuracy = 0.9929999709129333\n",
            "Epoch 28 \t| Training accuracy = 0.9940000176429749\n",
            "Epoch 29 \t| Training accuracy = 0.9940000176429749\n",
            "Epoch 30 \t| Training accuracy = 0.9940000176429749\n",
            "Epoch 31 \t| Training accuracy = 0.9940000176429749\n",
            "Epoch 32 \t| Training accuracy = 0.9940000176429749\n",
            "Epoch 33 \t| Training accuracy = 0.9940000176429749\n",
            "Epoch 34 \t| Training accuracy = 0.9940000176429749\n",
            "Epoch 35 \t| Training accuracy = 0.9940000176429749\n",
            "Epoch 36 \t| Training accuracy = 0.9940000176429749\n",
            "Epoch 37 \t| Training accuracy = 0.9940000176429749\n",
            "Epoch 38 \t| Training accuracy = 0.9940000176429749\n",
            "Epoch 39 \t| Training accuracy = 0.9940000176429749\n",
            "Epoch 40 \t| Training accuracy = 0.9940000176429749\n",
            "Epoch 41 \t| Training accuracy = 0.9940000176429749\n",
            "Epoch 42 \t| Training accuracy = 0.9940000176429749\n",
            "Epoch 43 \t| Training accuracy = 0.9940000176429749\n",
            "Epoch 44 \t| Training accuracy = 0.9940000176429749\n",
            "Epoch 45 \t| Training accuracy = 0.9940000176429749\n",
            "Epoch 46 \t| Training accuracy = 0.9940000176429749\n",
            "Epoch 47 \t| Training accuracy = 0.9940000176429749\n",
            "Epoch 48 \t| Training accuracy = 0.9940000176429749\n",
            "Epoch 49 \t| Training accuracy = 0.9940000176429749\n",
            "Training finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CnWu7tSVtQmn",
        "colab_type": "text"
      },
      "source": [
        "## Making Predictions!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-QjEAtmtYZx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e677043d-ab1a-4e24-dc5d-846b5824fec8"
      },
      "source": [
        "# Loading our test image!\n",
        "img = Image.open('/content/drive/My Drive/test_img.png')\n",
        "img\n",
        "\n",
        "# We will need to adjust our image given our trained formatting\n",
        "# Inverted grayscale\n",
        "# Flattening \n",
        "\n",
        "bw_image = img.convert(mode='L') # Grayscale reversed\n",
        "img_array = np.invert(bw_image) \n",
        "print('Current shape:', img_array.shape) # 28 x 28 --> We don't have the channel anymore given the grayscale conversion\n",
        "\n",
        "test_image = img_array.ravel() # Flattening array\n",
        "print('Image shape after flattening:', test_image.shape)"
      ],
      "execution_count": 477,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current shape: (28, 28)\n",
            "Image shape after flattening: (784,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MvJeE2uxvsD2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "daed970e-64fb-48af-e796-0fcf38c80010"
      },
      "source": [
        "# Calling our session to make the predictions\n",
        "\n",
        "# fetches => Takes the biggest row from our output\n",
        "# feed_dict => We just need to provide our image\n",
        "prediction = sess.run(fetches=tf.argmax(output, axis=1), feed_dict={X:[test_image]}) \n",
        "\n",
        "print('Prediction for test image is:', prediction)"
      ],
      "execution_count": 478,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction for test image is: [2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfPp6dHQtTLD",
        "colab_type": "text"
      },
      "source": [
        "## Testing and Evaltuation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfyPPdsWxDnw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a810d90f-6700-4498-ad5a-fb06aae203d6"
      },
      "source": [
        "# Checking the accuracu with our X test and y test datasets\n",
        "\n",
        "test_accuracy = sess.run(fetches=accuraccy, feed_dict={X: x_test_all, y: y_test_all})\n",
        "print(f'Accuracy on the test set is {test_accuracy:0.2%}')"
      ],
      "execution_count": 479,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on the test set is 97.89%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmANkYGvpNFI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reseting for the next run. These will 'clean' our calculations preventing anomalities\n",
        "train_writer.close()\n",
        "validation_writer.close()\n",
        "sess.close()\n",
        "tf.compat.v1.reset_default_graph()"
      ],
      "execution_count": 480,
      "outputs": []
    }
  ]
}